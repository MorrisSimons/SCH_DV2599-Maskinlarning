{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing data and libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from xgboost import XGBClassifier\n",
    "file_path = 'SpotifyFeatures.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vizualiing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get a sense of why the preprocessing steps are necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visulize \"duplicate songs\"(same trackid = same song)\n",
    "data['track_id'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display how many songs are \"duplicated\"(same trackid = same song) distrobution\n",
    "data['track_id'].value_counts().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample to Understand why there are duplicated songs\n",
    "data[data['track_id'] == '6AIte2Iej1QKlaofpjCzW1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prints the distribution of songs into all genres\n",
    "data['genre'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many songs are above 50 in popularity\n",
    "hits = data[data['popularity'] > 50].shape[0]\n",
    "miss = data[data['popularity'] < 50].shape[0]\n",
    "\n",
    "# visulize the distribution of popularity\n",
    "sns.histplot(data['popularity'], kde=True)\n",
    "plt.show()\n",
    "\n",
    "hits_ratio = hits/(hits+miss)*100\n",
    "print(f\"{hits_ratio:.3f}% of the songs are above 50 in popularity\")\n",
    "print(f\"with an average popularity of {data['popularity'].mean():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusions\n",
    "- data has songs that are to long we want 8.3 minutes to be max and 1 minute to be min because we want to measure normal songs\n",
    "- some attributes need to be transformed to numerical scale\n",
    "- A capella is to small to be considered\n",
    "- Children's Music and Children’s Music should be the same"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step number 1 Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First steps based on visualization\n",
    "- remove song that are to long in duration songs longer than 500000 ms (500 seconds = 8.3 minutes)\n",
    "- remove songs that are to short in duration songs shorter than 60000 ms (60 seconds = 1 minute)\n",
    "- Remove A Capella songs due to being to small\n",
    "- Combine union child genres into one genre Children's Music\n",
    "- Split songs into it's genres\n",
    "- Convert songs to Miss or Hit based on popularity\n",
    "\n",
    "#### Create new attribute\n",
    "- Genre count for each song with multilabeld genres\n",
    "\n",
    "#### The rest of the preprocessing steps\n",
    "- Pitch preprocessing\n",
    "- Time signature preprocessing\n",
    "- Mode preprocessing (minor/major)\n",
    "- Grouping songs by genre\n",
    "- Miss or Hit labeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop songs that are to long\n",
    "data = data[data['duration_ms'] < 500000]\n",
    "# Drop songs that are to short \n",
    "data = data[data['duration_ms'] > 60000]\n",
    "\n",
    "# drop gernre a capella\n",
    "data = data[data['genre'] != 'A Capella']\n",
    "\n",
    "# Combine union child genres\n",
    "data['genre'] = data['genre'].replace('Children’s Music', 'Children\\'s Music')\n",
    "\n",
    "# Create a new column for genre count\n",
    "#data['genre_count'] = data.groupby('track_id')['genre'].transform('count')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pitch preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert pitch to number\n",
    "def pitch_to_number(pitch):\n",
    "    pitch_map = {\n",
    "        'C': 0,\n",
    "        'C#': 1, 'Db': 1,\n",
    "        'D': 2,\n",
    "        'D#': 3, 'Eb': 3,\n",
    "        'E': 4, 'Fb': 4,\n",
    "        'E#': 5, 'F': 5,\n",
    "        'F#': 6, 'Gb': 6,\n",
    "        'G': 7,\n",
    "        'G#': 8, 'Ab': 8,\n",
    "        'A': 9,\n",
    "        'A#': 10, 'Bb': 10,\n",
    "        'B': 11, 'Cb': 11\n",
    "    }\n",
    "    return pitch_map.get(pitch, None)\n",
    "\n",
    "# Replace the pitch column with its numeric representation\n",
    "# Assuming the column name in your DataFrame that contains pitch values is 'pitch_column'\n",
    "data['key'] = data['key'].apply(pitch_to_number)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Time signature preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['time_signature'] = (data['time_signature'].apply(lambda x: x.split('/')[0])).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mode preprocessing (minor/major)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the mode column to 1 for major and 0 for minor\n",
    "data['mode'] = data['mode'].replace(\"Minor\", 0)\n",
    "data['mode'] = data['mode'].replace(\"Major\", 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grouping songs by genre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by genre and calculate mean popularity\n",
    "grouped = data.groupby('genre')\n",
    "mean_popularity = grouped['popularity'].mean()\n",
    "mean_popularity = mean_popularity.sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Miss or Hit labeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to label rows as 'hit' or 'MISS'\n",
    "def label_popularity(row, mean_popularity):\n",
    "    if row['popularity'] >= mean_popularity[row['genre']]:\n",
    "        return 'hit'\n",
    "    else:\n",
    "        return 'miss'\n",
    "\n",
    "# Apply the function to each row\n",
    "data['popularity_label'] = data.apply(lambda row: label_popularity(row, mean_popularity), axis=1)\n",
    "\n",
    "# Splitting the data into hit and miss, ensuring each genre is split 50/50\n",
    "# This step might require adjusting the labels for genres with an odd number of entries\n",
    "for genre in data['genre'].unique():\n",
    "    genre_data = data[data['genre'] == genre]\n",
    "    n = len(genre_data) // 2\n",
    "    popular_indices = genre_data.nlargest(n, 'popularity').index\n",
    "    nonpopular_indices = genre_data.nsmallest(n, 'popularity').index\n",
    "    data.loc[popular_indices, 'popularity_label'] = 'hit'\n",
    "    data.loc[nonpopular_indices, 'popularity_label'] = 'miss'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step number 2 Remaking the label popularity to Hit or Miss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summary of what will come\n",
    "- Visualize the distribution of the target variable in terms of genres\n",
    "- Visualize the distribution of hit and miss songs in terms of genres songs from spotify with a popularity over 50 and under 50\n",
    "- Visualize the distribution of the new 50/50 split to balance the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize the distribution of the target variable in terms of genres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the popularity of each genre and color children's music\n",
    "mean_popularity.plot(kind='bar', figsize=(15, 10))\n",
    "colors = ['red' if genre == 'Children\\'s Music' else 'skyblue' for genre in mean_popularity.index]\n",
    "plt.bar(mean_popularity.index, mean_popularity, color=colors)\n",
    "plt.title('Mean Popularity of Each Genre')\n",
    "plt.xlabel('Genre')\n",
    "plt.ylabel('Mean Popularity')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize the distribution of hit and miss songs in terms of genres songs from spotify with a popularity over 50 and under 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['popularity_over_50'] = data['popularity'] >= 50\n",
    "\n",
    "popularity_over_50_df = data.groupby(['genre', 'popularity_over_50']).size().unstack(fill_value=0)\n",
    "popularity_over_50_df['total'] = popularity_over_50_df.sum(axis=1)\n",
    "popularity_over_50_df_sorted = popularity_over_50_df.sort_values(by='total', ascending=False)\n",
    "popularity_over_50_df_sorted = popularity_over_50_df_sorted.drop(columns='total')\n",
    "\n",
    "# Plotting\n",
    "popularity_over_50_df_sorted.plot(kind='bar', stacked=True, figsize=(20, 10))\n",
    "plt.title('Popularity Over 50 by Genre')\n",
    "plt.xlabel('Genre')\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_dict_over50 = {}\n",
    "for genre in data['genre'].unique():\n",
    "    genre_data = data[data['genre'] == genre]\n",
    "    genre_data = genre_data.dropna()\n",
    "    X = genre_data[['acousticness', 'danceability', 'energy', 'instrumentalness', 'liveness', 'loudness', 'speechiness', 'tempo', 'valence']]\n",
    "    y = genre_data['popularity_over_50']\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    accuracy_dict_over50[genre] = accuracy\n",
    "    print(f'Accuracy for {genre}: {accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example of the performance when using popularity over 50 as a label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_dict_over50 = dict(sorted(accuracy_dict_over50.items(), key=lambda item: item[1], reverse=True))\n",
    "\n",
    "accuracy_df = pd.DataFrame(list(accuracy_dict_over50.items()), columns=['Genre', 'Accuracy'])\n",
    "\n",
    "accuracy_df.plot(kind='bar', x='Genre', y='Accuracy', figsize=(20, 10), legend=True)\n",
    "plt.title('Model Accuracy by Genre')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Genre')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize the distribution of the new 50/50 split to balance the dataset\n",
    "\n",
    "##### * unfair for taylor swift to compete with the best of less populat genres best artsits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "popularity_label_df = data.groupby(['genre', 'popularity_label']).size().unstack(fill_value=0)\n",
    "\n",
    "\n",
    "popularity_label_df['total'] = popularity_label_df.sum(axis=1)\n",
    "\n",
    "\n",
    "popularity_label_df_sorted = popularity_label_df.sort_values(by='total', ascending=False)\n",
    "popularity_label_df_sorted = popularity_label_df_sorted.drop(columns='total')\n",
    "\n",
    "popularity_label_df_sorted.plot(kind='bar', stacked=True, figsize=(20, 10))\n",
    "plt.title('Popularity Label by Genre')\n",
    "plt.xlabel('Genre')\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_dict_50percent = {}\n",
    "for genre in data['genre'].unique():\n",
    "    genre_data = data[data['genre'] == genre]\n",
    "    genre_data = genre_data.dropna()\n",
    "    X = genre_data[['acousticness', 'danceability', 'energy', 'instrumentalness', 'liveness', 'loudness', 'speechiness', 'tempo', 'valence']]\n",
    "    y = genre_data['popularity_label']\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    accuracy_dict_50percent[genre] = accuracy\n",
    "    print(f'Accuracy for {genre}: {accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example of the performance when using the new 50/50 split as a label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_dict_50percent = dict(sorted(accuracy_dict_50percent.items(), key=lambda item: item[1], reverse=True))\n",
    "\n",
    "accuracy_df = pd.DataFrame(list(accuracy_dict_50percent.items()), columns=['Genre', 'Accuracy'])\n",
    "\n",
    "# Plotting\n",
    "accuracy_df.plot(kind='bar', x='Genre', y='Accuracy', figsize=(20, 10), legend=True)\n",
    "plt.title('Model Accuracy by Genre')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Genre')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step number 3 Focusing on children's music genre and cleaning the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comments on genre selection to further analyze\n",
    "- By splitting the genres into it's own column we can now analyze the data in terms of genres this results in us having 25 datasets to analyze. To make the analysis easier to understand we will only analyze the biggest dataset children's music. \n",
    "\n",
    "#### Next steps\n",
    "- quick overview\n",
    "  - check for duplicates\n",
    "  - check avg popularity (to understand target variable)\n",
    "  - find weird values\n",
    "  - Analyze artist distribution (to understand if the artst with the most songs can make the dataset biased) # side note: Each artist has a different style so this could be a good thing\n",
    "- Analyze "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "childrens_music = data[data['genre'] == 'Children\\'s Music']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for songs with same track_id to find duplicates\n",
    "print(childrens_music[childrens_music['track_id'].duplicated()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find songs with bad values - like the sample below\n",
    "childrens_music[childrens_music['track_id'] == '7ARLbcqLgOrBI2JfzfKtHD']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace the $ in the track_name and artist_name columns with s\n",
    "childrens_music.loc[:, 'track_name'] = childrens_music['track_name'].str.replace('$', 's')\n",
    "childrens_music.loc[:, 'artist_name'] = childrens_music['artist_name'].str.replace('$', 's')\n",
    "\n",
    "childrens_music[childrens_music['track_id'] == '7ARLbcqLgOrBI2JfzfKtHD']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_songs = len(childrens_music)\n",
    " \n",
    "# Calculate the average popularity for each artist\n",
    "avg_popularity_by_artist = childrens_music.groupby('artist_name')['popularity'].mean()\n",
    "\n",
    "# Find the top 20 artists by the number of songs\n",
    "top_20_artists = childrens_music['artist_name'].value_counts().head(20)\n",
    "top_20_artists_df = top_20_artists.reset_index()\n",
    "top_20_artists_df.columns = ['artist_name', 'Song Count']\n",
    "\n",
    "# Merge the average popularity data with the top 20 artists DataFrame\n",
    "top_20_artists_with_avg_popularity = pd.merge(top_20_artists_df, avg_popularity_by_artist, on='artist_name')\n",
    "\n",
    "# Calculate the share of total songs for each of the top 20 artists and add it to the DataFrame\n",
    "top_20_artists_with_avg_popularity['% of songs'] = (top_20_artists_with_avg_popularity['Song Count'] / total_songs) * 100\n",
    "\n",
    "#'Share of Total Songs (%)' round 2 decimals\n",
    "top_20_artists_with_avg_popularity['popularity'] = top_20_artists_with_avg_popularity['popularity'].round(2)\n",
    "top_20_artists_with_avg_popularity['% of songs'] = top_20_artists_with_avg_popularity['% of songs'].round(2)\n",
    "\n",
    "print(top_20_artists_with_avg_popularity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the distribution of popularity for children's music\n",
    "sns.histplot(childrens_music['popularity'])\n",
    "plt.title('Popularity Distribution for Children\\'s Music')\n",
    "plt.xlabel('Popularity')\n",
    "plt.ylabel('Count')\n",
    "plt.show()\n",
    "print(childrens_music['popularity'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#visulize our new target attribute hit or miss metric to show that it is balanced\n",
    "sns.countplot(data['popularity_label'])\n",
    "plt.title('Popularity Label Distribution')\n",
    "plt.xlabel('Popularity Label')\n",
    "plt.ylabel('Count')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step number 4 Analyzing the children's music genre"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Plotting the distribution of the attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of the columns to plot with audio features, duration and genre count\n",
    "columns = ['acousticness', 'danceability', 'duration_ms', 'energy', 'instrumentalness', 'liveness', 'loudness', 'speechiness', 'tempo']\n",
    "\n",
    "# Create a figure and axis to plot on\n",
    "fig, ax = plt.subplots(5, 2, figsize=(20, 20))\n",
    "\n",
    "# Flatten the axis to make it easier to iterate over\n",
    "ax = ax.flatten()\n",
    "\n",
    "# Iterate over the columns and plot each one\n",
    "for i, col in enumerate(columns):\n",
    "    sns.histplot(childrens_music[col], kde=True, ax=ax[i])\n",
    "    ax[i].set_title(f'{col} Distribution')\n",
    "\n",
    "# Display the plots\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Popularity distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "childrens_music = data[data['genre'] == 'Children\\'s Music']\n",
    "# plot the distribution of popularity for children's music\n",
    "sns.histplot(childrens_music['popularity'])\n",
    "plt.title('Popularity Distribution for Children\\'s Music')\n",
    "plt.xlabel('Popularity')\n",
    "plt.ylabel('Count')\n",
    "plt.show()\n",
    "print(childrens_music['popularity'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dive deeper into instrumentalness\n",
    "instrumentalness = childrens_music['instrumentalness']\n",
    "instrumentalness.describe()\n",
    "\n",
    "# show the most common values for instrumentalness and how many songs are above 0\n",
    "instrumentalness.value_counts().head(10), childrens_music.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test a model to approximate most important features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run a test to see what feature is the most important\n",
    "# We will use the RandomForestClassifier for this as it is a good model for feature importance and maybe the model we will use for the final model\n",
    "\n",
    "# Create a new DataFrame with only the audio features and the popularity label\n",
    "X = childrens_music[['acousticness', 'danceability', 'energy', 'instrumentalness', 'liveness', 'loudness', 'speechiness', 'tempo']]\n",
    "y = childrens_music['popularity_label']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "model_0 = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "model_0.fit(X_train, y_train)\n",
    "y_pred = model_0.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importances = pd.DataFrame(model_0.feature_importances_, index=X_train.columns, columns=['importance']).sort_values('importance', ascending=False)\n",
    "print(\"feature importances\")\n",
    "print(feature_importances)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Conclusion on Children's Music dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Comments\n",
    "- The 2 biggest artists are contributing to 10% of the dataset which is not to much to make the dataset biased \n",
    "- The dataset need some scaling and normalization to be able to use it in a model\n",
    "- the target variable is now miss or hit and popularity is not needed anymore\n",
    "- The most import features can be viewed above.\n",
    "- The dataset is now free from weird values and duplicates\n",
    "\n",
    "##### Next steps\n",
    "- preform log transformation on the dataset to make it a tiny bit more normal distributed\n",
    "- split the dataset into a train and test set before scaling to prevent data leakage\n",
    "- scale the dataset before preforming PCA\n",
    "- preform PCA to reduce the dimensionality of the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Log transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preform logtransofrmation on livness, speechiness\n",
    "childrens_music['liveness'] = np.log1p(childrens_music['liveness'])\n",
    "childrens_music['speechiness'] = np.log1p(childrens_music['speechiness'])\n",
    "\n",
    "# Create a figure and axis to plot on\n",
    "fig, ax = plt.subplots(5, 2, figsize=(20, 20))\n",
    "\n",
    "# Flatten the axis to make it easier to iterate over\n",
    "ax = ax.flatten()\n",
    "\n",
    "# Iterate over the columns and plot each one\n",
    "for i, col in enumerate(columns):\n",
    "    sns.histplot(childrens_music[col], kde=True, ax=ax[i])\n",
    "    ax[i].set_title(f'{col} Distribution')\n",
    "\n",
    "# Display the plots\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# get data to csv\n",
    "childrens_music.to_csv('childrens_music.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What data do we want and what order do we want it\n",
    "X = childrens_music[['track_name', 'artist_name', 'track_id','mode', 'time_signature','acousticness', 'danceability', 'energy', 'instrumentalness', 'liveness', 'loudness', 'speechiness', 'tempo', 'duration_ms', 'valence']]\n",
    "y = childrens_music['popularity_label']\n",
    "\n",
    "# Split the data into a training and testing set for final model\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_features = ['mode', 'time_signature','acousticness', 'danceability', 'energy', 'instrumentalness', 'liveness', 'loudness', 'speechiness', 'tempo', 'valence']\n",
    "\n",
    "model_2 = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "model_2.fit(X_train[predict_features], y_train)\n",
    "\n",
    "y_pred = model_2.predict(X_test[predict_features])\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "feature_importances = pd.DataFrame(model_2.feature_importances_, index=predict_features, columns=['importance']).sort_values('importance', ascending=False)\n",
    "feature_importances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scaling selection\n",
    "Standard scaler: This technique transforms the features so they have the properties of a standard normal distribution with a mean of 0 and a standard deviation of 1. It's useful when your data follows a Gaussian distribution and when using algorithms sensitive to variance in the data, such as Support Vector Machines (SVMs) and Principal Component Analysis (PCA). From this we will exclude genre count."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Comments\n",
    "Standard scaler: This technique transforms the features so they have the properties of a standard normal distribution with a mean of 0 and a standard deviation of 1. It's useful when your data follows a Gaussian distribution and when using algorithms sensitive to variance in the data, such as Support Vector Machines (SVMs) and Principal Component Analysis (PCA). From this we will exclude genre count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Selecting the numerical features for standardization\n",
    "numerical_features = ['acousticness', 'danceability', 'duration_ms', 'energy', \n",
    "                      'instrumentalness', 'liveness', 'loudness', 'speechiness', 'tempo', 'valence']\n",
    "\n",
    "# Standardizing the numerical features\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit and transform the training data\n",
    "X_train[numerical_features] = scaler.fit_transform(X_train[numerical_features])\n",
    "\n",
    "# Transform the testing data\n",
    "X_test[numerical_features] = scaler.transform(X_test[numerical_features])\n",
    "\n",
    "X_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_features = ['mode', 'time_signature','acousticness', 'danceability', 'energy', 'instrumentalness', 'liveness', 'loudness', 'speechiness', 'tempo', 'valence']\n",
    "\n",
    "model_2 = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "model_2.fit(X_train[predict_features], y_train)\n",
    "\n",
    "y_pred = model_2.predict(X_test[predict_features])\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step number 5 PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Checking high correlation and if PCA is needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming X_train[features] is your scaled dataset for training\n",
    "features = ['acousticness', 'danceability', 'duration_ms', 'energy', \n",
    "            'instrumentalness', 'liveness', 'loudness', 'speechiness', 'tempo', 'valence', 'mode', 'time_signature']\n",
    "\n",
    "# Correlation analysis\n",
    "corr_matrix = np.corrcoef(X_train[features].T)\n",
    "sns.heatmap(corr_matrix, annot=True, annot_kws={\"size\": 7})\n",
    "\n",
    "plt.show()\n",
    "\n",
    "high_correlation_threshold = 0.7 # \n",
    "\n",
    "# Calculate the number of variables with at least one high correlation\n",
    "# We subtract the count by the length of the matrix to ignore the diagonal (self-correlation)\n",
    "high_correlations = np.sum((np.abs(corr_matrix) > high_correlation_threshold) & (corr_matrix != 1)) / 2\n",
    "\n",
    "\n",
    "# Decide whether PCA is recommended\n",
    "# This threshold can be adjusted based on the size of the matrix or domain knowledge\n",
    "pca_recommended = high_correlations > len(corr_matrix) * 0.5\n",
    "\n",
    "print(f\"PCA is {'recommended' if pca_recommended else 'not recommended'} based on the correlation matrix. There are {int(high_correlations)} pairs of highly correlated variables.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We decided to use PCA, now find the optimal number of components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Because we want to see the impact of the features on the target variable and we want to see if we can reduce the dimensionality of the dataset. But from the analysis we can see that the dataset is not to big so we might not need to use PCA and the correlation between the features are not to high so we might not need to use PCA. But we will still use it to see if we can reduce the dimensionality of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pca = PCA()\n",
    "pca.fit(X_train[features])\n",
    "\n",
    "# Calculating the cumulative explained variance ratio\n",
    "cumulative_explained_variance = np.cumsum(pca.explained_variance_ratio_)\n",
    "limit = 0.85\n",
    "\n",
    "# Plotting the cumulative explained variance to visualize the optimal number of components\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, len(cumulative_explained_variance) + 1), cumulative_explained_variance, marker='o', linestyle='--')\n",
    "plt.title('Cumulative Explained Variance by PCA Components')\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('Cumulative Explained Variance')\n",
    "plt.axhline(y=limit, color='r', linestyle='--', label='85% Explained Variance')\n",
    "plt.axvline(x=np.where(cumulative_explained_variance >= limit)[0][0] + 1, color='r', linestyle='--')\n",
    "plt.legend(loc='best')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# To find the exact number of components explaining 85% of the variance\n",
    "optimal_n_components = np.where(cumulative_explained_variance >= limit)[0][0] + 1\n",
    "print(f\"Optimal number of components to retain 85% of the variance: {optimal_n_components}\")\n",
    "\n",
    "# Now, fitting PCA again with the optimal number of components found\n",
    "pca_optimal = PCA(n_components=optimal_n_components)\n",
    "X_train_pca_optimal = pca_optimal.fit_transform(X_train[features])\n",
    "X_test_pca_optimal = pca_optimal.transform(X_test[features])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Applying PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If needed, creating a DataFrame for the PCA-transformed training data for easier analysis\n",
    "pca_columns = [f'PC{i+1}' for i in range(optimal_n_components)]\n",
    "pca_train_df = pd.DataFrame(X_train_pca_optimal, columns=pca_columns)\n",
    "\n",
    "# Similarly, creating a DataFrame for the PCA-transformed testing data\n",
    "pca_test_df = pd.DataFrame(X_test_pca_optimal, columns=pca_columns)\n",
    "\n",
    "# Display the first few rows of the PCA-transformed training DataFrame\n",
    "print(pca_train_df.head())\n",
    "\n",
    "# Additionally, to examine the variance explained by the chosen components\n",
    "explained_variance_ratio = pca.explained_variance_ratio_\n",
    "cumulative_variance = explained_variance_ratio.cumsum()\n",
    "\n",
    "print(\"Explained variance ratio by component:\", explained_variance_ratio)\n",
    "print(\"Cumulative variance explained:\", cumulative_variance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checking approxiate performance of PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model with PCA\n",
    "model_pca = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "model_pca.fit(X_train_pca_optimal, y_train)\n",
    "\n",
    "y_pred_pca = model_pca.predict(X_test_pca_optimal)\n",
    "\n",
    "print(classification_report(y_test, y_pred_pca))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification model selection\n",
    "\n",
    "#### Step 1. testing each model we want to use\n",
    "\n",
    "- Random Forest classifier\n",
    "- Support Vector Machine\n",
    "- XGBoost\n",
    "- Logistic Regression \n",
    "- Decision Tree Classifier\n",
    "- K-Nearest Neighbors Classifier\n",
    "\n",
    "\n",
    "#### Step 2. Comparing the models with cross validation\n",
    "\n",
    "#### Step 3. Hyperparameter tuning and cross validation to compare again\n",
    "\n",
    "#### Step 4. Final comparison and plotting the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#our datasets\n",
    "# The datasets with PCA components\n",
    "X_train_pca_optimal, X_test_pca_optimal\n",
    "\n",
    "# The datasets without PCA components\n",
    "X_train_normal = X_train[features]\n",
    "X_test_normal = X_test[features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert miss or hit to 0 or 1 (some models require this) on the y_train and y_test\n",
    "y_train = y_train.replace({'miss': 0, 'hit': 1})\n",
    "y_test = y_test.replace({'miss': 0, 'hit': 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With PCA\n",
    "SVC_Model = SVC()\n",
    "SVC_Model.fit(X_train_pca_optimal, y_train)\n",
    "SVC_Predict = SVC_Model.predict(X_test_pca_optimal)\n",
    "SVC_Accuracy = accuracy_score(y_test, SVC_Predict)\n",
    "print(\"Accuracy: \" + str(SVC_Accuracy))\n",
    "SVC_AUC = roc_auc_score(y_test, SVC_Predict)\n",
    "print(\"AUC: \" + str(SVC_AUC))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVC_Model = SVC()\n",
    "SVC_Model.fit(X_train_normal, y_train)\n",
    "SVC_Predict = SVC_Model.predict(X_test_normal)\n",
    "SVC_Accuracy = accuracy_score(y_test, SVC_Predict)\n",
    "print(\"Accuracy: \" + str(SVC_Accuracy))\n",
    "SVC_AUC = roc_auc_score(y_test, SVC_Predict)\n",
    "print(\"AUC: \" + str(SVC_AUC))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# whit PCA\n",
    "DT_Model = DecisionTreeClassifier()\n",
    "DT_Model.fit(X_train_pca_optimal, y_train)\n",
    "DT_Predict = DT_Model.predict(X_test_pca_optimal)\n",
    "DT_Accuracy = accuracy_score(y_test, DT_Predict)\n",
    "print(\"Accuracy: \" + str(DT_Accuracy))\n",
    "DT_AUC = roc_auc_score(y_test, DT_Predict)\n",
    "print(\"AUC: \" + str(DT_AUC))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DT_Model = DecisionTreeClassifier()\n",
    "DT_Model.fit(X_train_normal, y_train)\n",
    "DT_Predict = DT_Model.predict(X_test_normal)\n",
    "DT_Accuracy = accuracy_score(y_test, DT_Predict)\n",
    "print(\"Accuracy: \" + str(DT_Accuracy))\n",
    "DT_AUC = roc_auc_score(y_test, DT_Predict)\n",
    "print(\"AUC: \" + str(DT_AUC))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RandomizedSearchCV Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_cross_vals = 5\n",
    "set_iter_vals = 75\n",
    "random_seed = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the classifier\n",
    "rfc = RandomForestClassifier()\n",
    "\n",
    "# Define the parameter distribution\n",
    "param_dist = {\n",
    "    'n_estimators': [100, 200, 250, 300, 400, 500],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'bootstrap': [True, False]\n",
    "}\n",
    "\n",
    "# Initialize the RandomizedSearchCV object for accuracy optimization\n",
    "random_search = RandomizedSearchCV(estimator=rfc, param_distributions=param_dist, n_iter=set_iter_vals, cv=set_cross_vals, scoring='accuracy', verbose=2, n_jobs=-1, random_state=random_seed)\n",
    "\n",
    "# Fit the random search to the data\n",
    "random_search.fit(X_train_normal, y_train)\n",
    "\n",
    "# Best parameters found\n",
    "print(\"Best Parameters: \", random_search.best_params_)\n",
    "rfc__best_params = random_search.best_params_\n",
    "# Predictions\n",
    "RFC_Predict = random_search.predict(X_test_normal)\n",
    "\n",
    "# Evaluate the model for accuracy\n",
    "RFC_accuracy = accuracy_score(y_test, RFC_Predict)\n",
    "print(\"Accuracy: \" + str(RFC_accuracy))\n",
    "\n",
    "# Evaluate the model for AUC\n",
    "rfc_auc = roc_auc_score(y_test, RFC_Predict)\n",
    "print(\"AUC: \" + str(rfc_auc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the SVC model\n",
    "svc_model = SVC()\n",
    "\n",
    "# Specify the parameter distribution\n",
    "param_distributions = {\n",
    "    'C': np.logspace(-2, 3, 6),  \n",
    "    'kernel': ['rbf', 'sigmoid'],  # Including 'sigmoid' kernel\n",
    "    'gamma': ['scale', 'auto'],  # Kernel coefficient for 'rbf', 'poly', and 'sigmoid'\n",
    "}\n",
    "\n",
    "# Initialize RandomizedSearchCV\n",
    "random_search = RandomizedSearchCV(\n",
    "    svc_model,\n",
    "    param_distributions=param_distributions,\n",
    "    n_iter=set_iter_vals,  # Number of parameter settings that are sampled\n",
    "    cv=set_cross_vals,  # Cross-validation folding strategy\n",
    "    scoring='accuracy',  # Scoring method\n",
    "    random_state=42,\n",
    "    verbose=2,\n",
    "    n_jobs=-1  # Use all available cores\n",
    ")\n",
    "\n",
    "# Fit RandomizedSearchCV to the PCA-transformed training data\n",
    "random_search.fit(X_train_normal, y_train)\n",
    "\n",
    "# Print the best parameters\n",
    "print(\"Best Parameters:\", random_search.best_params_)\n",
    "SVC__best_params = random_search.best_params_\n",
    "# Predict with the best estimator\n",
    "SVC_predict = random_search.predict(X_test_normal)\n",
    "\n",
    "# Evaluate the model with the optimized parameters\n",
    "SVC_accuracy = accuracy_score(y_test, SVC_predict)\n",
    "print(\"Accuracy:\", SVC_accuracy)\n",
    "svc_auc = roc_auc_score(y_test, SVC_predict)\n",
    "print(\"AUC:\", svc_auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the XGBoost model\n",
    "xgb_model = XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
    "\n",
    "# Specify the parameter distribution\n",
    "param_distributions = {\n",
    "    'n_estimators': [100,120, 200, 300, 400, 500],\n",
    "    'learning_rate': [0.01, 0.04, 0.05, 0.1, 0.2],\n",
    "    'max_depth': [3, 4, 5, 6, 7, 8],\n",
    "    'min_child_weight': [1, 5, 10],\n",
    "    'gamma': [0.5, 1, 1.5, 2, 5],\n",
    "    'subsample': [0.6, 0.8, 1.0],\n",
    "    'colsample_bytree': [0.6, 0.8, 1.0],\n",
    "}\n",
    "\n",
    "# Initialize RandomizedSearchCV\n",
    "random_search = RandomizedSearchCV(\n",
    "    xgb_model,\n",
    "    param_distributions=param_distributions,\n",
    "    n_iter=set_iter_vals,  # Adjust based on your computational budget\n",
    "    cv=set_cross_vals,\n",
    "    scoring='accuracy',\n",
    "    random_state=random_seed,\n",
    "    verbose=2,\n",
    "    n_jobs=-1  # Use all available cores\n",
    ")\n",
    "\n",
    "# Fit RandomizedSearchCV to the training data\n",
    "random_search.fit(X_train_normal, y_train)\n",
    "\n",
    "# Print the best parameters\n",
    "print(\"Best Parameters:\", random_search.best_params_)\n",
    "XBG__best_params = random_search.best_params_\n",
    "# Predict with the best estimator\n",
    "XGB_predict = random_search.predict(X_test_normal)\n",
    "\n",
    "# Evaluate the model with the optimized parameters\n",
    "xgb_accuracy = accuracy_score(y_test, XGB_predict)\n",
    "print(\"Accuracy:\", xgb_accuracy)\n",
    "xgb_auc = roc_auc_score(y_test, XGB_predict)\n",
    "print(\"AUC:\", xgb_auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Logistic Regression model\n",
    "lr_model = LogisticRegression()\n",
    "\n",
    "# Specify the parameter distributions\n",
    "param_distributions = {\n",
    "    'C': np.logspace(-4, 4, 20),  # Wide range of values for C\n",
    "    'penalty': ['l1', 'l2'],  # Common penalties for logistic regression\n",
    "    'solver': ['liblinear', 'saga'],\n",
    "    'max_iter': [100, 1000, 10000]  # Different values for maximum iterations\n",
    "}\n",
    "# 'liblinear' works well with 'l1' and 'l2' penalties.\n",
    "lr_model.solver = 'liblinear'\n",
    "\n",
    "# Initialize RandomizedSearchCV\n",
    "random_search = RandomizedSearchCV(\n",
    "    lr_model,\n",
    "    param_distributions=param_distributions,\n",
    "    n_iter=set_cross_vals,  # Number of parameter settings sampled\n",
    "    cv=set_iter_vals,  # 5-fold cross-validation\n",
    "    scoring='accuracy',  # Scoring metric to optimize\n",
    "    random_state=random_seed,\n",
    "    verbose=2,\n",
    "    n_jobs=-1  # Use all available cores\n",
    ")\n",
    "\n",
    "# Fit RandomizedSearchCV to the training data\n",
    "random_search.fit(X_train_normal, y_train)\n",
    "\n",
    "# Print the best parameters\n",
    "print(\"Best Parameters:\", random_search.best_params_)\n",
    "\n",
    "# Predict with the best estimator\n",
    "lr_predict = random_search.predict(X_test_normal)\n",
    "lr__best_params = random_search.best_params_\n",
    "# Evaluate the model with the optimized parameters\n",
    "LR_accuracy = accuracy_score(y_test, lr_predict)\n",
    "print(\"Accuracy:\", LR_accuracy)\n",
    "lr_auc = roc_auc_score(y_test, lr_predict)\n",
    "print(\"AUC:\", lr_auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Decision Tree Classifier\n",
    "dt_model = DecisionTreeClassifier()\n",
    "\n",
    "# Specify the parameter distributions\n",
    "param_distributions = {\n",
    "    'max_depth': [None, 10, 20, 30, 40, 50],\n",
    "    'min_samples_split': np.arange(2, 11),\n",
    "    'min_samples_leaf': np.arange(1, 11),\n",
    "    'max_features': ['sqrt', 'log2', None] + list(range(1, X_train_normal.shape[1] + 1)),\n",
    "    'criterion': ['gini', 'entropy']\n",
    "}\n",
    "\n",
    "\n",
    "# Initialize RandomizedSearchCV\n",
    "random_search = RandomizedSearchCV(\n",
    "    dt_model,\n",
    "    param_distributions=param_distributions,\n",
    "    n_iter=set_iter_vals,  # Number of parameter settings sampled\n",
    "    cv=set_cross_vals,  # 5-fold cross-validation\n",
    "    scoring='accuracy',  # Scoring metric to optimize\n",
    "    random_state=random_seed,\n",
    "    verbose=2,\n",
    "    n_jobs=-1  # Use all available cores\n",
    ")\n",
    "\n",
    "# Fit RandomizedSearchCV to the training data\n",
    "random_search.fit(X_train_normal, y_train)\n",
    "dt_best_params = random_search.best_params_\n",
    "# Print the best parameters\n",
    "print(\"Best Parameters:\", random_search.best_params_)\n",
    "\n",
    "# Predict with the best estimator\n",
    "dt_predict = random_search.predict(X_test_normal)\n",
    "\n",
    "# Evaluate the model with the optimized parameters\n",
    "DT_accuracy = accuracy_score(y_test, dt_predict)\n",
    "print(\"Accuracy:\", DT_accuracy)\n",
    "dt_auc = roc_auc_score(y_test, dt_predict)\n",
    "print(\"AUC:\", dt_auc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### KNears Neighbors Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the K-Nearest Neighbors Classifier\n",
    "knn_model = KNeighborsClassifier()\n",
    "\n",
    "# Specify the parameter distributions\n",
    "param_distributions = {\n",
    "    'n_neighbors': np.arange(1, 200),  # Exploring a wide range of neighbors\n",
    "    'weights': ['uniform', 'distance'],\n",
    "    'p': [1, 2]\n",
    "}\n",
    "\n",
    "# Initialize RandomizedSearchCV\n",
    "random_search = RandomizedSearchCV(\n",
    "    knn_model,\n",
    "    param_distributions=param_distributions,\n",
    "    n_iter=set_iter_vals,  # Number of parameter settings sampled\n",
    "    cv=set_cross_vals,  # 5-fold cross-validation\n",
    "    scoring='accuracy',  # Scoring metric to optimize\n",
    "    random_state=random_seed,\n",
    "    verbose=2,\n",
    "    n_jobs=-1  # Use all available cores, note that KNN can be slow for large datasets\n",
    ")\n",
    "\n",
    "# Fit RandomizedSearchCV to the training data\n",
    "random_search.fit(X_train_normal, y_train)\n",
    "\n",
    "# Print the best parameters\n",
    "print(\"Best Parameters:\", random_search.best_params_)\n",
    "knn_best_params = random_search.best_params_\n",
    "# Predict with the best estimator\n",
    "knn_predict = random_search.predict(X_test_normal)\n",
    "\n",
    "# Evaluate the model with the optimized parameters\n",
    "KNN_accuracy = accuracy_score(y_test, knn_predict)\n",
    "print(\"Accuracy:\", KNN_accuracy)\n",
    "knn_auc = roc_auc_score(y_test, knn_predict)\n",
    "print(\"AUC:\", knn_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the models\n",
    "rfc_final = RandomForestClassifier(n_estimators=rfc__best_params['n_estimators'], max_depth=rfc__best_params['max_depth'], min_samples_split=rfc__best_params['min_samples_split'], min_samples_leaf=rfc__best_params['min_samples_leaf'], bootstrap=rfc__best_params['bootstrap'])\n",
    "svc_final = SVC(C=SVC__best_params['C'], kernel=SVC__best_params['kernel'], gamma=SVC__best_params['gamma'])\n",
    "xgb_final = XGBClassifier(n_estimators=XBG__best_params['n_estimators'], learning_rate=XBG__best_params['learning_rate'], max_depth=XBG__best_params['max_depth'], min_child_weight=XBG__best_params['min_child_weight'], gamma=XBG__best_params['gamma'], subsample=XBG__best_params['subsample'], colsample_bytree=XBG__best_params['colsample_bytree'])\n",
    "lr_final = LogisticRegression(C=lr__best_params['C'], penalty=lr__best_params['penalty'], solver=lr__best_params['solver'], max_iter=lr__best_params['max_iter'])\n",
    "dt_final = DecisionTreeClassifier(max_depth=dt_best_params['max_depth'], min_samples_split=dt_best_params['min_samples_split'], min_samples_leaf=dt_best_params['min_samples_leaf'], max_features=dt_best_params['max_features'], criterion=dt_best_params['criterion'])\n",
    "knn_final = KNeighborsClassifier(n_neighbors=knn_best_params['n_neighbors'], weights=knn_best_params['weights'], p=knn_best_params['p'])\n",
    "\n",
    "# Cross-validation setup\n",
    "cv = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "# Function to run experiments for a single model and collect scores\n",
    "def run_experiment(model, X, y, cv):\n",
    "    scores = cross_val_score(model, X, y, cv=cv, scoring='accuracy')\n",
    "    return scores\n",
    "\n",
    "# Run the experiments for each model separately\n",
    "# Replace 'X_test_normal' and 'y_test' with your actual datasets\n",
    "rfc_scores = run_experiment(rfc_final, X_test_normal, y_test, cv)\n",
    "svc_scores = run_experiment(svc_final, X_test_normal, y_test, cv)\n",
    "xgb_scores = run_experiment(xgb_final, X_test_normal, y_test, cv)\n",
    "lr_scores = run_experiment(lr_final, X_test_normal, y_test, cv)\n",
    "dt_scores = run_experiment(dt_final, X_test_normal, y_test, cv)\n",
    "knn_scores = run_experiment(knn_final, X_test_normal, y_test, cv)\n",
    "\n",
    "# Combine the scores into a DataFrame for ranking\n",
    "scores_df = pd.DataFrame({\n",
    "    \"RandomForestClassifier\": rfc_scores,\n",
    "    \"SVC\": svc_scores,\n",
    "    \"XGBClassifier\": xgb_scores,\n",
    "    \"LogisticRegression\": lr_scores,\n",
    "    \"DecisionTreeClassifier\": dt_scores,\n",
    "    \"KNeighborsClassifier\": knn_scores\n",
    "})\n",
    "\n",
    "# Rank the models based on their performance in each fold\n",
    "ranks_df = scores_df.rank(axis=1, method='average', ascending=False)\n",
    "\n",
    "# Calculate the average rank for each model across all folds\n",
    "average_ranks = ranks_df.mean().sort_values()\n",
    "\n",
    "print(\"Average Ranks:\")\n",
    "print(average_ranks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the mean of the score of each model\n",
    "rfc_mean = rfc_scores.mean()\n",
    "svc_mean = svc_scores.mean()\n",
    "xgb_mean = xgb_scores.mean()\n",
    "lr_mean = lr_scores.mean()\n",
    "dt_mean = dt_scores.mean()\n",
    "knn_mean = knn_scores.mean()\n",
    "\n",
    "# Create a DataFrame with models and their mean scores\n",
    "accuracy_ranking = pd.DataFrame({\n",
    "    'Model': ['RFC', 'SVC', 'XGB', 'LR', 'DT', 'KNN'],\n",
    "    'Mean Accuracy': [rfc_mean, svc_mean, xgb_mean, lr_mean, dt_mean, knn_mean]\n",
    "})\n",
    "\n",
    "# Rank models based on Mean Accuracy\n",
    "accuracy_ranking['Rank'] = accuracy_ranking['Mean Accuracy'].rank(ascending=False)\n",
    "\n",
    "# Calculate average rank (in this case, it's the same as Rank since we have only one metric)\n",
    "accuracy_ranking = accuracy_ranking.sort_values(by='Rank')\n",
    "\n",
    "# Print the DataFrame\n",
    "print(accuracy_ranking)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_accuracy = accuracy_ranking.iloc[0]['Model']\n",
    "\n",
    "print(f\"The best model based on accuracy is {best_model_accuracy} with an accuracy of {accuracy_ranking.iloc[-1]['Mean Accuracy']:.3f}\")\n",
    "\n",
    "color_accuracy = ['skyblue' if model != best_model_accuracy else 'red' for model in accuracy_ranking['Model']]\n",
    "\n",
    "# plot \n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(12, 3))\n",
    "\n",
    "accuracy_ranking.plot(kind='bar', x='Model', y='Mean Accuracy', ax=ax, legend=False, color=color_accuracy)\n",
    "ax.set_title('Model Accuracy Score')\n",
    "ax.set_ylabel('Accuracy')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# list format for the best model\n",
    "after_opt_acc = accuracy_ranking\n",
    "\n",
    "after_opt_acc, average_ranks"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
